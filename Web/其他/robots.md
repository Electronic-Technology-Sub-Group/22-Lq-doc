`robots.txt` 文件用于控制搜索引擎爬虫对网站的抓取规范，规定哪些部分可以被爬取，哪些需要忽略。

相关教程参考 [Robots.txt 简介与指南](https://developers.google.com/search/docs/crawling-indexing/robots/intro?hl=zh-cn) 与 [Google 如何解读 robots.txt 规范](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt?hl=zh-cn)

#未完成 